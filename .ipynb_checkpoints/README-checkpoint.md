<p align="center">
  <img src="images/Triton_Deployemt_Banner.png" alt="Triton_Deployemt_Banner" width="100%">
</p>

# ğŸ§  Triton Latency Benchmark Suite  
**Optimizing PyTorch Inference with NVIDIA Triton + Performance Visualization**

Built by **Dartayous** â€” a Hollywood VFX veteran turned AI Engineer â€” this project benchmarks native PyTorch inference against NVIDIA Triton's production-grade deployment pipeline.

---

## ğŸš€ Highlights

- ğŸ“¦ Deployed a PyTorch CNN (`simple_cnn`) using NVIDIA Triton
- âš™ï¸ Generated TensorRT engine and configured model repository
- ğŸ“ˆ Benchmarked with `perf_analyzer` (latency + throughput)
- ğŸ“Š Visualized performance metrics via `matplotlib`
- ğŸ†š Compared Triton vs raw PyTorch latency
- ğŸ” CLI automation for repeatable benchmarking
- ğŸ§¹ Cleaned repo structure with `.gitignore` and size-optimized tracking
- ğŸŒ GitHub-ready: structured, documented, and deployable

---

## ğŸ“ Project Structure

```text
ai_deployment_day17/
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ perf_results.csv           â† Triton benchmark output
â”‚   â”œâ”€â”€ pytorch_baseline.txt       â† Native PyTorch latency baseline
â”‚   â””â”€â”€ plot_perf_vs_latency.png   â† Visualization of inference latency
â”œâ”€â”€ model_repository/
â”‚   â””â”€â”€ simple_cnn/
â”‚       â”œâ”€â”€ config.pbtxt           â† Triton model configuration
â”‚       â””â”€â”€ 1/
â”‚           â””â”€â”€ model.plan         â† TensorRT engine
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ measure_pytorch_baseline.py  â† Raw PyTorch benchmark runner
â”‚   â””â”€â”€ triton_latency_bench.py      â† Triton latency CLI tool + plotter
â””â”€â”€ README.md


## ğŸ§ª Quick Start
1ï¸âƒ£ Benchmark PyTorch Inference
python scripts/measure_pytorch_baseline.py
Runs 100 inferences using ResNet18 and logs the average latency to pytorch_baseline.txt.


2ï¸âƒ£ Launch Triton SDK Container
Run from your project root:
docker run --gpus all -it --rm --net=host ^
  -v ${PWD}:/workspace ^
  nvcr.io/nvidia/tritonserver:25.06-py3-sdk

Inside the container, install dependencies:
pip3 install matplotlib pandas


3ï¸âƒ£ Run Triton Benchmark & Visualize Results
Inside the container:
python3 scripts/triton_latency_bench.py

This will:

Run perf_analyzer with concurrency levels 1â€“4

Save results to benchmarks/perf_results.csv

Generate a plot: plot_perf_vs_latency.png

Overlay raw PyTorch latency as a baseline


## ğŸ“Š Visualization Sample
ğŸ”´ Red dashed line: native PyTorch latency

ğŸ”µ Blue curve: Triton inference performance across concurrent loads

ğŸ§¼ Git & Repository Notes
.gitignore includes:
__pycache__/
*.png
*.csv
*.txt
*.plan
.env/
venv/


Oversized files removed from repo:

cnn_model.pth (98 MB)

onnx_model.onnx (98 MB)

torch_cpu.dll (240 MB)

dnnl.lib (675 MB)

venv/ â€” excluded entirely

ğŸ”— Users should download models separately or retrain locally.


## ğŸ§° Tech Stack
ğŸ Python 3.x

ğŸ”¬ PyTorch + torchvision

ğŸš€ NVIDIA Triton Inference Server

âš¡ TensorRT

ğŸ³ Docker (SDK image: nvcr.io/nvidia/tritonserver:25.06-py3-sdk)

ğŸ“ˆ CLI & visualization: perf_analyzer, matplotlib, pandas

## ğŸ¬ Author
Dartayous
ğŸï¸ Hollywood VFX professional turned AI Engineer
ğŸ§  Specializing in deployment pipelines, performance optimization, and multimodal AI
ğŸ”§ Architect of this end-to-end benchmark suite


âœ¨ Future Enhancements
Add argparse support to CLI tools

Timestamped benchmark logging

Web dashboard for real-time visualization

GitHub Actions: CI for benchmark validation

## ğŸ’¡ About This Journey
This project evolved incrementally:

âœ… Started with raw PyTorch inference

ğŸš€ Deployed with Triton + TensorRT engine generation

ğŸ” Integrated SDK tools to measure latency and throughput

ğŸ” Added CLI automation + visualization

ğŸ§¼ Cleaned commit history and optimized structure for public release